# ğŸ”¥ FlexLLM: A Composable HLS Library for Rapid LLM Accelerator Design

FlexLLM is a **composable High-Level Synthesis (HLS) library** for rapidly building **hybrid temporalâ€“spatial accelerators** for Large Language Models (LLMs).  
It provides parameterized module templates, optimized memory-access/dataflow components, and a complete quantization suite, enabling FPGA-based LLM systems to be built with **minimal manual engineering effort**.

Using FlexLLM, we implemented a **full Llama-3.2-1B inference system**â€”including prefill, decode, tokenizer integration, and long-context memoryâ€”**in under two months with ~1K lines of code**.

---

## âœ¨ Key Features

- **Composable HLS Library** for LLM accelerator development  
- **Hybrid Temporalâ€“Spatial Architecture**  
- **Hardware-Efficient Quantization Suite**  
- **Hierarchical Memory Transformer (HMT) Plug-In**  
- **FPGA Deployment Ready**

---

## ğŸ“Š Performance Summary

### AMD U280 FPGA (16nm) vs. NVIDIA A100 GPU (7nm)
- 1.29Ã— end-to-end speedup  
- 1.64Ã— higher decode throughput  
- 3.14Ã— better energy efficiency  

### Projected V80 FPGA (7nm)
- 4.71Ã— end-to-end speedup  
- 6.55Ã— decode throughput  
- 4.13Ã— energy efficiency  

### Long-Context (with HMT)
- 23.23Ã— reduced prefill latency  
- 64Ã— longer context window  

---

## ğŸ“ Repository Layout

```
FlexLLM/
â”œâ”€ Modules/                          # Core FlexLLM module library (compute, quant, memory, data movement)
â”‚
â”œâ”€ SpinQuant_Llama_32_1B_Ins/        # Llama-3.2-1B-Instruct accelerator (SpinQuant)
â”‚  â”œâ”€ parameters/                    # Downloaded model parameters
â”‚  â”œâ”€ RapidStream_pref_u280/         # Prefill RapidStream config (U280)
â”‚  â”œâ”€ RapidStream_dec_u280/          # Decode RapidStream config (U280)
â”‚  â”œâ”€ run/                           # Bitstreams, hosts, and test scripts
â”‚  â”‚  â”œâ”€ bitstreams/                 # FPGA .xclbin files
â”‚  â”‚  â”œâ”€ parameters/                 # Downloaded parameters
â”‚  â”‚  â”œâ”€ llama-3.2-1b-f16.gguf       # Tokenizer (download required)
â”‚  â”‚  â”œâ”€ SpinQuant_Prefilling_Decoding_mem_opt
â”‚  â”‚  â”œâ”€ SpinQuant_Prefilling_Decoding_mem_opt_demo
â”‚  â”‚  â””â”€ test files (.py/.txt/.csv)
â”‚  â””â”€ TAPA files                     # TAPA HLS kernels, host code, memory configs
â”‚
â”œâ”€ SpinQuant_Llama_32_1B/            # Llama-3.2-1B accelerator (SpinQuant)
â”œâ”€ HMT_SpinQuant_Llama_32_1B/        # Llama-3.2-1B-Instruct + SpinQuant + HMT
â””â”€ README.md
```

---

## ğŸ“¦ Download Required Files

Download parameters & GGUF from:

https://drive.google.com/drive/folders/149QLnEm-NT3fhCgB4Uy7Xda1oLu2zk-7?usp=sharing

Place them in:

```
FlexLLM/your_model/parameters/
FlexLLM/your_model/run/parameters/
FlexLLM/your_model/run/llama-3.2-1b-f16.gguf
```

---

## ğŸ§° Requirements

- Ubuntu 20.04 / 22.04  
- XRT installed  
- Vitis 2022.2  
- TAPA CLI  
- Compatible FPGA board  

Check FPGA:

```
xbutil examine
```

---

## ğŸ›  Build (Host Only)

```
export FLEXLLM_HOME=/path/to/FlexLLM
export LLAMA_CPP_ROOT=/path/to/llama.cpp

tapa g++ -- SpinQuant_Prefilling_Decoding_mem_opt_demo.cpp   -I$FLEXLLM_HOME/Modules   -I$LLAMA_CPP_ROOT   -I$LLAMA_CPP_ROOT/include   -I$LLAMA_CPP_ROOT/ggml/include   -I$LLAMA_CPP_ROOT/ggml/include/ggml   $LLAMA_CPP_ROOT/build/bin/libllama.so   -Wl,-rpath,$LLAMA_CPP_ROOT/build/bin   -lpthread -ldl -lm   -o run/SpinQuant_Prefilling_Decoding_mem_opt_demo
```

---

## ğŸš€ Run on U280

```
./SpinQuant_Prefilling_Decoding_mem_opt_demo   --bitstream_pref bitstreams/SpinQuant_Prefilling_mem_opt_xilinx_u280_gen3x16_xdma_1_202211_1.xclbin   --bitstream_dec  bitstreams/SpinQuant_Decoding_mem_opt_xilinx_u280_gen3x16_xdma_1_202211_1.xclbin   llama-3.2-1b-f16.gguf my_prompt.txt my_answer.txt
```

---

## ğŸ“ Notes for V80 Support

V80 results are estimates. Full bitstreams coming soon.

---

## ğŸ™ Acknowledgments

We thank AMD â€” **Fraser Nicholas** and **Michaela Blott** â€” for support and guidance.